{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "joseph-annamkutty-bot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Xn2b-fiiRa",
        "outputId": "147ee97b-eeef-49a4-a749-1b9883216004"
      },
      "source": [
        "import torch\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7dBvq9twQkp"
      },
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI0zG3Tnw67W"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnXexp1g1puu"
      },
      "source": [
        "                                            \n",
        "                                            \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVF9xGMBxqwb",
        "outputId": "97448506-05a9-4d17-bf85-b5e6de7fcf69"
      },
      "source": [
        "!kaggle datasets download -d manann/quotes-500k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading quotes-500k.zip to /content\n",
            " 90% 81.0M/89.6M [00:01<00:00, 48.6MB/s]\n",
            "100% 89.6M/89.6M [00:01<00:00, 62.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL8-EQ-p1F5G",
        "outputId": "5e9e4c42-9503-4ebd-ef11-2b8306c93b01"
      },
      "source": [
        "!unzip quotes-500k.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  quotes-500k.zip\n",
            "  inflating: quotes.csv              \n",
            "  inflating: test.txt                \n",
            "  inflating: train.txt               \n",
            "  inflating: valid.txt               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHb5wpazkMVt"
      },
      "source": [
        "## Prepare Quotes\n",
        "\n",
        "Only quotes with tags 'inspirational','reality','optimism','life','motivational' are chosen for training the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmp8lLOt2z70"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZECAf7M3DtvA",
        "outputId": "4be55b93-e530-44ce-b823-f12cd842128c"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulprxkNt14pW"
      },
      "source": [
        "data = pd.read_csv('quotes.csv')\n",
        "d = data[:50000]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rDVD_Oc21vC"
      },
      "source": [
        "#extract motivational quotes from the dataset\n",
        "\n",
        "inspire = pd.DataFrame()\n",
        "entry = pd.DataFrame()\n",
        "tags = ['inspirational','reality','optimism','life','motivational']\n",
        "for i in range (50000):\n",
        "  for tag in tags:\n",
        "    if(tag in d.iloc[[i],[2]].to_string()):\n",
        "      if(len(d.iloc[[i],[2]].to_string().split(','))>2):\n",
        "        taglist = d.iloc[[i],[2]].to_string().split(',')[1]\n",
        "        entry['tag'] = [taglist]\n",
        "        text = d['quote'][i]\n",
        "        entry['quote'] = [text]\n",
        "        inspire=inspire.append(entry)\n",
        "        break\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAN-hbJG27WY",
        "outputId": "af951989-4ffa-4c99-b296-f1c6e8e37e1a"
      },
      "source": [
        "print(inspire.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13871, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4e9IFywBfFS"
      },
      "source": [
        "inspire.to_csv('bot.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj0neeyxF8Bx"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEAYCP0Neo8j"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOFt0fYIA4zB"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_cosine_with_hard_restarts_schedule_with_warmup, top_k_top_p_filtering\n",
        "import warnings\n",
        "import torch.nn.functional as F\n",
        "import math, random\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rq4tI_bEDFH"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data_file_name, tokenizer, seq_length=64):\n",
        "    super().__init__()\n",
        "    data_path = os.path.join(data_file_name)\n",
        "    self.examples = []\n",
        "    \n",
        "    \n",
        "    tag_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "    quote_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "    pad_tkn = tokenizer.pad_token_id\n",
        "    eos_tkn = tokenizer.eos_token_id\n",
        "\n",
        "    with open(data_path) as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      \n",
        "      for row in csv_reader:\n",
        "        tag = [tag_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-1)\n",
        "        quote = [quote_tkn] + tokenizer.encode(row[2],max_length=seq_length//2-2) + [eos_tkn]\n",
        "        tokens = tag + quote + [pad_tkn] * ( seq_length - len(tag) - len(quote) )\n",
        "        segments = [tag_tkn] * len(tag) + [quote_tkn] * ( seq_length - len(tag) )\n",
        "        labels = [-100] * (len(tag)+1) + quote[1:] + [-100] * ( seq_length - len(tag) - len(quote) )\n",
        "        self.examples.append((tokens, segments, labels))\n",
        "\n",
        "        \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "      \n",
        "  def __getitem__(self, item):\n",
        "    \n",
        "    return torch.tensor(self.examples[item])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZgKrxz-it09"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def fit(model, optimizer, train_dl, val_dl, epochs, device):\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    print('\\n--- Starting epoch #{} ---'.format(i))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # These 2 lists will keep track of the batch losses and batch sizes over one epoch:\n",
        "    losses = []\n",
        "    nums = []\n",
        "\n",
        "    for xb in tqdm(train_dl, desc=\"Training\"):\n",
        "      # Move the batch to the training device:\n",
        "      inputs = xb.to(device)\n",
        "\n",
        "      # Call the model with the token ids, segment ids, and the ground truth (labels)\n",
        "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
        "      \n",
        "      # Add the loss and batch size to the list:\n",
        "      loss = outputs[0]\n",
        "      losses.append(loss.item())\n",
        "      nums.append(len(xb))\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "    # Compute the average cost over one epoch:\n",
        "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "\n",
        "    # Now do the same thing for validation:\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      losses = []\n",
        "      nums = []\n",
        "\n",
        "      for xb in tqdm(val_dl, desc=\"Validation\"):\n",
        "        inputs = xb.to(device)\n",
        "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
        "        losses.append(outputs[0].item())\n",
        "        nums.append(len(xb))\n",
        "\n",
        "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqt5yjsgPwmd",
        "outputId": "aa3dff67-425d-49c2-94df-cceadbc67f8c"
      },
      "source": [
        "import math, random\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "dataset = MyDataset('bot.csv',tokenizer)\n",
        "# Create data indices for training and validation splits:\n",
        "\n",
        "indices = list(range(len(dataset)))\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "\n",
        "split = math.floor(0.1 * len(dataset))\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Build the PyTorch data loaders:\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n",
        "val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
        "# Note: we can double the batch size for validation since no backprogation is involved (thus it will fit on GPU's memory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ch_LfKzYpkv"
      },
      "source": [
        "\n",
        "print ('Loading/Downloading GPT-2 Model')\n",
        "TOKENIZER = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "MODEL = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfVQTaXhJGjC",
        "outputId": "b92ea69e-4be9-4c2b-b7ec-aca4be9aa2c4"
      },
      "source": [
        "SPECIAL_TOKENS_DICT = {\n",
        "    'pad_token': '<pad>',\n",
        "    'additional_special_tokens': ['<tag>', '<quote>'],\n",
        "}\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_STEPS = 4\n",
        "MAX_SEQ_LEN = 32\n",
        "MODEL_NAME = 'bot'\n",
        "DATA_FILE = 'bot.csv'\n",
        "TOKENIZER.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
        "MODEL.resize_token_embeddings(len(TOKENIZER))\n",
        "#TRAIN_LOADER, VAL_LOADER = get_data_loader(DATA_FILE, TOKENIZER)\n",
        "DEVICE = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "\tDEVICE = 'cuda'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading/Downloading GPT-2 Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfwZyPyXPlv4"
      },
      "source": [
        "from transformers import AdamW\n",
        "MODEL.to(DEVICE)\n",
        "\n",
        "# Fine-tune GPT2 for two epochs:\n",
        "optimizer = AdamW(MODEL.parameters())\n",
        "model = fit(MODEL, OPTIMIZER, train_loader, val_loader, epochs=2, device=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttRd3I9biBQt"
      },
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size x vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "# From HuggingFace, adapted to work with the context/slogan separation:\n",
        "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=20, top_p=0.8, repetition_penalty=5,\n",
        "                    device='cpu'):\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            if segments_tokens != None:\n",
        "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
        "\n",
        "\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # greedy sampling:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSLjEfDsiG3n",
        "outputId": "7ff8d44b-4a82-48cb-8b40-7a0088e9ffbb"
      },
      "source": [
        "tag = \"love\"\n",
        "\n",
        "tag_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "quote_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "\n",
        "input_ids = [tag_tkn] + tokenizer.encode(tag)\n",
        "\n",
        "segments = [quote_tkn] * 64\n",
        "\n",
        "segments[:len(input_ids)] = [tag_tkn] * len(input_ids)\n",
        "\n",
        "input_ids += [quote_tkn]\n",
        "\n",
        "# Move the model back to the CPU for inference:\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "# Generate 20 samples of max length 20\n",
        "generated = sample_sequence(model, length=50, context=input_ids, segments_tokens=segments, num_samples=10)\n",
        "\n",
        "print('\\n\\n--- Generated Slogans ---\\n')\n",
        "for g in generated:\n",
        "  quote = tokenizer.decode(g.squeeze().tolist())\n",
        "  quote = quote.split('<|endoftext|>')[0].split('<quote>')[1]\n",
        "  print(quote)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:35<00:00,  1.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--- Generated Slogans ---\n",
            "\n",
            " Love is like a magnet. The more you know, the greater love increases exponentially when it flows and becomes invisible as well with time again\n",
            " I love him, my friend. And I like to keep it and he always gets a lot of pleasure in me too but sometimes that is what really\n",
            " Love is like water. It flows and you can't see it coming in when there are no paths or even where the waves go until your friends leave\n",
            " We are the people we love, and when there comes our heart becomes ours. We can be in harmony with each other if it is not right to\n",
            " It is not always easy to love others who don't have a history. But once you know the difference between what one sees in each other, that\n",
            " I don't know what to do with people. I can think about all of the things in life, and if you have something that makes them feel\n",
            " When you're in love, don't be afraid to let yourself down.\n",
            " Love is not a competition. It's what you get, and it doesn't mean that people are always in love or even at the end of your\n",
            " I have the ability to see and appreciate what I feel for myself, even though my heart is full of pain.\n",
            " I love you. I'm the kind of person who's been through everything for a while now, so let me keep moving on and see what everyone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwArbinJhSNV"
      },
      "source": [
        "torch.save(model.state_dict(), \"bot.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZRTJv_t4fzPK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}